{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model Notebook\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wildan Mufid R\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Wildan Mufid R\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import h5py\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "ROWS_PER_FRAME = 543\n",
    "FIXED_FRAMES = 34\n",
    "\n",
    "RH_IDX = 501\n",
    "LH_IDX = 522\n",
    "POSE_IDX = 468\n",
    "FACE_IDX = 0\n",
    "\n",
    "lips_UpperOuter = [185, 40, 39, 37, 0, 267, 269, 270, 409]\n",
    "lips_LowerOuter = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291]\n",
    "lips_UpperInner = [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308]\n",
    "lips_LowerInner = [191, 80, 81, 82, 13, 312, 311, 310, 415]\n",
    "LIPS_IDX = np.concatenate(\n",
    "    [lips_UpperOuter, lips_LowerOuter, lips_UpperInner, lips_LowerInner]\n",
    ")\n",
    "\n",
    "UPPER_BODY_IDX = np.arange(0, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_detection(frame, mp_model):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame.flags.writeable = False\n",
    "    landmarks = mp_model.process(frame)\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zsc(data, mean, std):\n",
    "    return (data - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_landmark(landmarks, faceIDX = np.arange(0,468), poseIDX = np.arange(0,33), component = ['face', 'pose', 'right_hand', 'left_hand']):\n",
    "    with h5py.File('param.h5','r') as hf:\n",
    "    \n",
    "        #Face\n",
    "        if 'face' in component:\n",
    "            if landmarks.face_landmarks:            \n",
    "                face = np.array([[landmarks.face_landmarks.landmark[idx].x, \n",
    "                                landmarks.face_landmarks.landmark[idx].y, \n",
    "                                landmarks.face_landmarks.landmark[idx].z] \n",
    "                                for idx in faceIDX])\n",
    "            else:\n",
    "                face = np.zeros((len(faceIDX),3))\n",
    "            \n",
    "            face = np.array(zsc(\n",
    "                face.T, np.array(hf.get('face/mean'))[:,faceIDX], np.array(hf.get('face/std'))[:,faceIDX]\n",
    "            )).T.flatten()\n",
    "        else:\n",
    "            face = [None] * (len(faceIDX) * 3)\n",
    "\n",
    "        #Pose\n",
    "        if 'pose' in component:\n",
    "            if landmarks.pose_landmarks:            \n",
    "                pose = np.array([[landmarks.pose_landmarks.landmark[idx].x, \n",
    "                                landmarks.pose_landmarks.landmark[idx].y, \n",
    "                                landmarks.pose_landmarks.landmark[idx].z,\n",
    "                                landmarks.pose_landmarks.landmark[idx].visibility] \n",
    "                                for idx in poseIDX])\n",
    "            else:\n",
    "                pose = np.zeros((len(poseIDX),4))\n",
    "            \n",
    "            pose = np.array(zsc(\n",
    "                pose.T, np.array(hf.get('pose/mean'))[:,poseIDX], np.array(hf.get('pose/std'))[:,poseIDX]\n",
    "            )).T.flatten()\n",
    "        else:\n",
    "            pose = [None] * (len(poseIDX) * 4)\n",
    "        \n",
    "        #Right Hand\n",
    "        if 'right_hand' in component:\n",
    "            if landmarks.right_hand_landmarks:            \n",
    "                rh = np.array([[cord.x, cord.y, cord.z] for cord in landmarks.right_hand_landmarks.landmark])\n",
    "            else:\n",
    "                rh = np.zeros((21,3))\n",
    "            \n",
    "            rh = np.array(zsc(\n",
    "                rh.T, np.array(hf.get('right_hand/mean')), np.array(hf.get('right_hand/std'))\n",
    "            )).T.flatten()\n",
    "        else:\n",
    "            rh = [None] * (63)\n",
    "        \n",
    "        #Left Hand\n",
    "        if 'left_hand' in component:\n",
    "            if landmarks.left_hand_landmarks:            \n",
    "                lh = np.array([[cord.x, cord.y, cord.z] for cord in landmarks.left_hand_landmarks.landmark])\n",
    "            else:\n",
    "                lh = np.zeros((21,3))\n",
    "            \n",
    "            lh = np.array(zsc(\n",
    "                lh.T, np.array(hf.get('left_hand/mean')), np.array(hf.get('left_hand/std'))\n",
    "            )).T.flatten()\n",
    "        else:\n",
    "            lh = [None] * (63)\n",
    "        \n",
    "        result = np.concatenate([face,pose,rh,lh])\n",
    "    return result[result != np.array(None)].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = {0: 'Batuk',\n",
    "           1: 'Demam',\n",
    "           2: 'Gigi',\n",
    "           3: 'Kepala',\n",
    "           4: 'Minum',\n",
    "           5: 'Obat',\n",
    "           6: 'Perut',\n",
    "           7: 'Resep',\n",
    "           8: 'Sakit'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_perf(model, input):\n",
    "    start = time.perf_counter()\n",
    "    result = model.predict(input)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    return result, elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = [\n",
    "    'models1/64/lstm_dset1_0.0001.h5',\n",
    "    'models3/64/lstm_dset2_0.0001.h5',\n",
    "    'models1/64/lstm_dset3_0.0001.h5',\n",
    "    'models3/16/lstm_dset4_0.0001.h5', \n",
    "    'models2/32/lstm_dset5_0.0001.h5',\n",
    "]\n",
    "\n",
    "bilstm = [\n",
    "    'models1/32/bilstm_dset1_0.0001.h5',\n",
    "    'models2/64/bilstm_dset2_0.0001.h5',\n",
    "    'models3/v2/64/bilstm_dset3_0.0001.h5',\n",
    "    'models2/32/bilstm_dset4_0.0001.h5',\n",
    "    'models3/32/bilstm_dset5_0.001.h5',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test/age'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAIN_PATH = f'test/{os.listdir(\"test/\")[0]}'\n",
    "MAIN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test/age/Batuk.MOV'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fPATH = f'{MAIN_PATH}/{os.listdir(MAIN_PATH)[0]}'\n",
    "fPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained w/ Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.load_model(lstm[0], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})\n",
    "model2 = tf.keras.models.load_model(bilstm[0], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 15s 15s/step\n",
      "perdict: Batuk \n",
      "elapsed: 14.970698899999661s\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "perdict: Batuk \n",
      "elapsed: 1.2157256000000416s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_landmark = []\n",
    "total_frame = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=.5, min_tracking_confidence=.5) as holistic_model:\n",
    "    cap = cv2.VideoCapture(fPATH)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "            \n",
    "        mp_results = mp_detection(frame, holistic_model)\n",
    "        sequence_landmark.append(preprocessing_landmark(mp_results))\n",
    "        total_frame += 1\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "sequence_landmark = np.expand_dims(np.array(sequence_landmark), axis=0)\n",
    "if total_frame > FIXED_FRAMES:\n",
    "    selected_idx = np.linspace(0, total_frame-1, FIXED_FRAMES, dtype=int)\n",
    "    sequence_landmark = sequence_landmark[:,selected_idx,:]\n",
    "elif total_frame < FIXED_FRAMES:\n",
    "    sequence_landmark = torch.from_numpy(np.array(sequence_landmark))\n",
    "    sequence_landmark = F.interpolate(sequence_landmark.permute(0,2,1), size=(FIXED_FRAMES), mode= 'nearest-exact').permute(0,2,1).numpy()\n",
    "\n",
    "result, elapsed = predict_perf(model1, sequence_landmark)\n",
    "print(f'perdict: {decoder[result.argmax()]} \\nelapsed: {elapsed}s\\n')\n",
    "result, elapsed = predict_perf(model2, sequence_landmark)\n",
    "print(f'perdict: {decoder[result.argmax()]} \\nelapsed: {elapsed}s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained w/ Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.load_model(lstm[1], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})\n",
    "model2 = tf.keras.models.load_model(bilstm[1], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "perdict: demam \n",
      "elapsed: 1.8414400000037858s\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "perdict: demam \n",
      "elapsed: 2.1403086000063922s\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "perdict: demam \n",
      "elapsed: 1.128532400005497s\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "perdict: sakit \n",
      "elapsed: 2.079010699999344s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_landmark = []\n",
    "total_frame = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=.5, min_tracking_confidence=.5) as holistic_model:\n",
    "    cap = cv2.VideoCapture(fPATH)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "            \n",
    "        mp_results = mp_detection(frame, holistic_model)\n",
    "        sequence_landmark.append(preprocessing_landmark(mp_results, faceIDX=LIPS_IDX))\n",
    "        total_frame += 1\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "sequence_landmark = np.expand_dims(np.array(sequence_landmark), axis=0)\n",
    "if total_frame > FIXED_FRAMES:\n",
    "    selected_idx = np.linspace(0, total_frame-1, FIXED_FRAMES, dtype=int)\n",
    "    sequence_landmark = sequence_landmark[:,selected_idx,:]\n",
    "elif total_frame < FIXED_FRAMES:\n",
    "    sequence_landmark = torch.from_numpy(np.array(sequence_landmark))\n",
    "    sequence_landmark = F.interpolate(sequence_landmark.permute(0,2,1), size=(FIXED_FRAMES), mode= 'nearest-exact').permute(0,2,1).numpy()\n",
    "\n",
    "result, elapsed = predict_perf(model1, sequence_landmark)\n",
    "print(f'perdict: {decoder[result.argmax()]} \\nelapsed: {elapsed}s\\n')\n",
    "result, elapsed = predict_perf(model2, sequence_landmark)\n",
    "print(f'perdict: {decoder[result.argmax()]} \\nelapsed: {elapsed}s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained w/ Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.load_model(lstm[2], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})\n",
    "model2 = tf.keras.models.load_model(bilstm[2], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "perdict: kepala \n",
      "elapsed: 1.2758823000040138s\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "perdict: demam \n",
      "elapsed: 2.1433032999993884s\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "perdict: kepala \n",
      "elapsed: 1.1492311999973026s\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "perdict: obat \n",
      "elapsed: 2.127744099998381s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_landmark = []\n",
    "total_frame = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=.5, min_tracking_confidence=.5) as holistic_model:\n",
    "    cap = cv2.VideoCapture(fPATH)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "            \n",
    "        mp_results = mp_detection(frame, holistic_model)\n",
    "        sequence_landmark.append(preprocessing_landmark(mp_results, poseIDX=UPPER_BODY_IDX))\n",
    "        total_frame += 1\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "sequence_landmark = np.expand_dims(np.array(sequence_landmark), axis=0)\n",
    "if total_frame > FIXED_FRAMES:\n",
    "    selected_idx = np.linspace(0, total_frame-1, FIXED_FRAMES, dtype=int)\n",
    "    sequence_landmark = sequence_landmark[:,selected_idx,:]\n",
    "elif total_frame < FIXED_FRAMES:\n",
    "    sequence_landmark = torch.from_numpy(np.array(sequence_landmark))\n",
    "    sequence_landmark = F.interpolate(sequence_landmark.permute(0,2,1), size=(FIXED_FRAMES), mode= 'nearest-exact').permute(0,2,1).numpy()\n",
    "\n",
    "result, elapsed = predict_perf(model1, sequence_landmark)\n",
    "print(f'perdict: {decoder[result.argmax()]} \\nelapsed: {elapsed}s\\n')\n",
    "result, elapsed = predict_perf(model2, sequence_landmark)\n",
    "print(f'perdict: {decoder[result.argmax()]} \\nelapsed: {elapsed}s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained w/ Dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.load_model(lstm[3], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})\n",
    "model2 = tf.keras.models.load_model(bilstm[3], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "perdict: batuk \n",
      "elapsed: 1.4101594000021578s\n",
      "\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "perdict: batuk \n",
      "elapsed: 2.687711599999602s\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "perdict: batuk \n",
      "elapsed: 1.1585707000012917s\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "perdict: batuk \n",
      "elapsed: 2.2014583999989554s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_landmark = []\n",
    "total_frame = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=.5, min_tracking_confidence=.5) as holistic_model:\n",
    "    cap = cv2.VideoCapture(fPATH)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "            \n",
    "        mp_results = mp_detection(frame, holistic_model)\n",
    "        sequence_landmark.append(preprocessing_landmark(mp_results, LIPS_IDX, UPPER_BODY_IDX))\n",
    "        total_frame += 1\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "sequence_landmark = np.expand_dims(np.array(sequence_landmark), axis=0)\n",
    "if total_frame > FIXED_FRAMES:\n",
    "    selected_idx = np.linspace(0, total_frame-1, FIXED_FRAMES, dtype=int)\n",
    "    sequence_landmark = sequence_landmark[:,selected_idx,:]\n",
    "elif total_frame < FIXED_FRAMES:\n",
    "    sequence_landmark = torch.from_numpy(np.array(sequence_landmark))\n",
    "    sequence_landmark = F.interpolate(sequence_landmark.permute(0,2,1), size=(FIXED_FRAMES), mode= 'nearest-exact').permute(0,2,1).numpy()\n",
    "\n",
    "\n",
    "result, elapsed = predict_perf(model1, sequence_landmark)\n",
    "print(f'perdict: {decoder[result.argmax()]} \\nelapsed: {elapsed}s\\n')\n",
    "result, elapsed = predict_perf(model2, sequence_landmark)\n",
    "print(f'perdict: {decoder[result.argmax()]} \\nelapsed: {elapsed}s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained w/ Dataset 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.load_model(lstm[4], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})\n",
    "model2 = tf.keras.models.load_model(bilstm[4], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "perdict: gigi \n",
      "elapsed: 1.3343352999982017s\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "perdict: gigi \n",
      "elapsed: 2.3062443000017083s\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "perdict: gigi \n",
      "elapsed: 1.1710155000000668s\n",
      "\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "perdict: gigi \n",
      "elapsed: 2.547043799997482s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sequence_landmark = []\n",
    "total_frame = 0\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=.5, min_tracking_confidence=.5) as holistic_model:\n",
    "    cap = cv2.VideoCapture(fPATH)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "            \n",
    "        mp_results = mp_detection(frame, holistic_model)\n",
    "        sequence_landmark.append(preprocessing_landmark(mp_results, component=['right_hand','left_hand']))\n",
    "        total_frame += 1\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "sequence_landmark = np.expand_dims(np.array(sequence_landmark), axis=0)\n",
    "if total_frame > FIXED_FRAMES:\n",
    "    selected_idx = np.linspace(0, total_frame-1, FIXED_FRAMES, dtype=int)\n",
    "    sequence_landmark = sequence_landmark[:,selected_idx,:]\n",
    "elif total_frame < FIXED_FRAMES:\n",
    "    sequence_landmark = torch.from_numpy(np.array(sequence_landmark))\n",
    "    sequence_landmark = F.interpolate(sequence_landmark.permute(0,2,1), size=(FIXED_FRAMES), mode= 'nearest-exact').permute(0,2,1).numpy()\n",
    "\n",
    "result, elapsed = predict_perf(model1, sequence_landmark)\n",
    "print(f'perdict: {decoder[result.argmax()]} \\nelapsed: {elapsed}s\\n')\n",
    "result, elapsed = predict_perf(model2, sequence_landmark)\n",
    "print(f'perdict: {decoder[result.argmax()]} \\nelapsed: {elapsed}s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "\n",
    "for mod in np.arange(0,5):\n",
    "    model3 = tf.keras.models.load_model(lstm[mod], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})\n",
    "    true = 0\n",
    "    \n",
    "    for p in os.listdir(\"test\"):\n",
    "        temp = []\n",
    "        for vid in os.listdir(f'test/{p}'):\n",
    "            fPATH = f'test/{p}/{vid}'\n",
    "            sequence_landmark = []\n",
    "            total_frame = 0\n",
    "\n",
    "            with mp_holistic.Holistic(min_detection_confidence=.5, min_tracking_confidence=.5) as holistic_model:\n",
    "                cap = cv2.VideoCapture(fPATH) \n",
    "                \n",
    "                while True:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret: break\n",
    "                        \n",
    "                    mp_results = mp_detection(frame, holistic_model)\n",
    "                    \n",
    "                    if mod == 0:\n",
    "                        sequence_landmark.append(preprocessing_landmark(mp_results))\n",
    "                    elif mod == 1:\n",
    "                        sequence_landmark.append(preprocessing_landmark(mp_results, faceIDX=LIPS_IDX))\n",
    "                    elif mod == 2:\n",
    "                        sequence_landmark.append(preprocessing_landmark(mp_results, poseIDX=UPPER_BODY_IDX))\n",
    "                    elif mod == 3:\n",
    "                        sequence_landmark.append(preprocessing_landmark(mp_results, LIPS_IDX, UPPER_BODY_IDX))\n",
    "                    elif mod == 4:\n",
    "                        sequence_landmark.append(preprocessing_landmark(mp_results, component=['right_hand','left_hand']))\n",
    "\n",
    "                    total_frame += 1\n",
    "                    \n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "            sequence_landmark = np.expand_dims(np.array(sequence_landmark), axis=0)\n",
    "            if total_frame > FIXED_FRAMES:\n",
    "                selected_idx = np.linspace(0, total_frame-1, FIXED_FRAMES, dtype=int)\n",
    "                sequence_landmark = sequence_landmark[:,selected_idx,:]\n",
    "            elif total_frame < FIXED_FRAMES:\n",
    "                sequence_landmark = torch.from_numpy(np.array(sequence_landmark))\n",
    "                sequence_landmark = F.interpolate(sequence_landmark.permute(0,2,1), size=(FIXED_FRAMES), mode= 'nearest-exact').permute(0,2,1).numpy()   \n",
    "            \n",
    "            result, elapsed = predict_perf(model3, sequence_landmark)\n",
    "            \n",
    "            if decoder[result.argmax()] == vid.split('.')[0]:\n",
    "                true += 1\n",
    "            \n",
    "    #         temp.append(decoder[result.argmax()])\n",
    "    #     res.append(temp)\n",
    "    # print(res)\n",
    "    acc.append(true/90)\n",
    "    \n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "\n",
    "for mod in np.arange(0,4):\n",
    "    model3 = tf.keras.models.load_model(bilstm[mod], custom_objects={'Addons>F1Score': tfa.metrics.F1Score(9)})\n",
    "    \n",
    "    temp = []\n",
    "    for p in os.listdir(\"test\"):\n",
    "        \n",
    "        for vid in os.listdir(f'test/{p}'):\n",
    "            fPATH = f'test/{p}/{vid}'\n",
    "            sequence_landmark = []\n",
    "            total_frame = 0\n",
    "\n",
    "            with mp_holistic.Holistic(min_detection_confidence=.5, min_tracking_confidence=.5) as holistic_model:\n",
    "                cap = cv2.VideoCapture(fPATH) \n",
    "                \n",
    "                while True:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret: break\n",
    "                        \n",
    "                    mp_results = mp_detection(frame, holistic_model)\n",
    "                    \n",
    "                    if mod == 0:\n",
    "                        sequence_landmark.append(preprocessing_landmark(mp_results))\n",
    "                    elif mod == 1:\n",
    "                        sequence_landmark.append(preprocessing_landmark(mp_results, faceIDX=LIPS_IDX))\n",
    "                    elif mod == 2:\n",
    "                        sequence_landmark.append(preprocessing_landmark(mp_results, poseIDX=UPPER_BODY_IDX))\n",
    "                    elif mod == 3:\n",
    "                        sequence_landmark.append(preprocessing_landmark(mp_results, LIPS_IDX, UPPER_BODY_IDX))\n",
    "                    elif mod == 4:\n",
    "                        sequence_landmark.append(preprocessing_landmark(mp_results, component=['right_hand','left_hand']))\n",
    "\n",
    "                    total_frame += 1\n",
    "                    \n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "            sequence_landmark = np.expand_dims(np.array(sequence_landmark), axis=0)\n",
    "            if total_frame > FIXED_FRAMES:\n",
    "                selected_idx = np.linspace(0, total_frame-1, FIXED_FRAMES, dtype=int)\n",
    "                sequence_landmark = sequence_landmark[:,selected_idx,:]\n",
    "            elif total_frame < FIXED_FRAMES:\n",
    "                sequence_landmark = torch.from_numpy(np.array(sequence_landmark))\n",
    "                sequence_landmark = F.interpolate(sequence_landmark.permute(0,2,1), size=(FIXED_FRAMES), mode= 'nearest-exact').permute(0,2,1).numpy()   \n",
    "            \n",
    "            result, elapsed = predict_perf(model3, sequence_landmark)\n",
    "            temp.append(decoder[result.argmax()])\n",
    "            \n",
    "    res.append(temp)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in res:\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
