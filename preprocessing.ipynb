{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**How to Read `landmark_id`**\n",
    ">\n",
    ">landmark_id structure: {`frame`}-{`landmark_type`}-{`landmark index`}\n",
    ">\n",
    ">\n",
    ">**Example:** `42-left_hand-20`\n",
    ">\n",
    ">Frame → 42<br>\n",
    ">Landmark Type → Left Hand<br>\n",
    ">Landmark Index → 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Index Range of Landmark Type**\n",
    ">\n",
    "> * Face: `0-467`\n",
    "> * Pose: `468-500`\n",
    "> * Right Hand: `501-521`\n",
    "> * Left Hand: `522-542`\n",
    ">\n",
    "> Landmark order each frame: **|** [`IDX:0`] Face → Pose → Right Hand → Left Hand [`IDX:542`] **|** [`IDX:543`]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wildan Mufid R\\AppData\\Local\\Temp\\ipykernel_23792\\1464568829.py:14: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  tqdm.tnrange(2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeadc0d486694d5eb0596964bdbbc59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tqdm.notebook.tqdm_notebook at 0x2185684a920>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import h5py\n",
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Dummy to trigger tqdm\n",
    "tqdm.tnrange(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS_PER_FRAME = 543\n",
    "FIXED_FRAMES = 34\n",
    "\n",
    "RH_IDX = 501\n",
    "LH_IDX = 522\n",
    "POSE_IDX = 468\n",
    "FACE_IDX = 0\n",
    "\n",
    "lips_UpperOuter = [185, 40, 39, 37, 0, 267, 269, 270, 409]\n",
    "lips_LowerOuter = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291]\n",
    "lips_UpperInner = [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308]\n",
    "lips_LowerInner = [191, 80, 81, 82, 13, 312, 311, 310, 415]\n",
    "LIPS_IDX = np.concatenate(\n",
    "    [lips_UpperOuter, lips_LowerOuter, lips_UpperInner, lips_LowerInner]\n",
    ")\n",
    "\n",
    "UPPER_BODY_IDX = np.arange(468, 493)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Source of Lips Index from Mediapipe Face Landmark: https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_landmark/tensors_to_face_landmarks_with_attention.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_reduction(pq_path,\n",
    "                       faceIDX = None, poseIDX = None,\n",
    "                       rhIDX = None, lhIDX = None):\n",
    "\n",
    "    df = pd.read_parquet(pq_path).drop(columns=['landmark_id'])\n",
    "    total_frame = int(len(df) / ROWS_PER_FRAME)\n",
    "\n",
    "    sequence_landmark=[]\n",
    "    for frame in range(total_frame):\n",
    "        boundary = ROWS_PER_FRAME * frame\n",
    "\n",
    "        #Face\n",
    "        if faceIDX:\n",
    "            face = df.iloc[faceIDX+boundary,:-1].to_numpy().flatten()\n",
    "        else:\n",
    "            face = [None] * (468 * 3)\n",
    "\n",
    "        #Pose\n",
    "        if poseIDX:\n",
    "            pose = df.iloc[poseIDX+boundary].to_numpy().flatten()\n",
    "        else:\n",
    "            pose = [None] * (33 * 4)\n",
    "\n",
    "        #Right Hand\n",
    "        if rhIDX:\n",
    "            rh = df.iloc[rhIDX+boundary,:-1].to_numpy().flatten()\n",
    "        else:\n",
    "            rh = [None] * (21 * 3)\n",
    "\n",
    "        #Left Hand\n",
    "        if lhIDX:\n",
    "            lh = df.iloc[lhIDX+boundary,:-1].to_numpy().flatten()\n",
    "        else:\n",
    "            lh = [None] * (21 * 3)\n",
    "\n",
    "        result = np.concatenate([face,pose,rh,lh])\n",
    "        sequence_landmark.append(result[result != np.array(None)].astype('float'))\n",
    "\n",
    "    return sequence_landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_reduction(pq_path,\n",
    "                       faceIDX = np.arange(0,468), poseIDX = np.arange(468,501),\n",
    "                       rhIDX = np.arange(501,522), lhIDX = np.arange(522,543),\n",
    "                       component = ['face', 'pose', 'right_hand', 'left_hand']):\n",
    "\n",
    "    df = pd.read_parquet(pq_path).drop(columns=['landmark_id'])\n",
    "    total_frame = int(len(df) / ROWS_PER_FRAME)\n",
    "\n",
    "    sequence_landmark=[]\n",
    "    for frame in range(total_frame):\n",
    "        boundary = ROWS_PER_FRAME * frame\n",
    "\n",
    "        #Face\n",
    "        if 'face' in component:\n",
    "            face = df.iloc[faceIDX+boundary,:-1].to_numpy().flatten()\n",
    "        else:\n",
    "            face = [None] * (len(faceIDX) * 3)\n",
    "\n",
    "        #Pose\n",
    "        if 'pose' in component:\n",
    "            pose = df.iloc[poseIDX+boundary].to_numpy().flatten()\n",
    "        else:\n",
    "            pose = [None] * (len(poseIDX) * 4)\n",
    "\n",
    "        #Right Hand\n",
    "        if 'right_hand' in component:\n",
    "            rh = df.iloc[rhIDX+boundary,:-1].to_numpy().flatten()\n",
    "        else:\n",
    "            rh = [None] * (len(rhIDX) * 3)\n",
    "\n",
    "        #Left Hand\n",
    "        if 'left_hand' in component:\n",
    "            lh = df.iloc[lhIDX+boundary,:-1].to_numpy().flatten()\n",
    "        else:\n",
    "            lh = [None] * (len(lhIDX) * 3)\n",
    "\n",
    "        result = np.concatenate([face,pose,rh,lh])\n",
    "        sequence_landmark.append(result[result != np.array(None)].astype('float'))\n",
    "\n",
    "    return sequence_landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_equalization(seq_data, FIXED_FRAMES):\n",
    "\n",
    "    total_frame = len(seq_data)\n",
    "    sequence_landmark = np.expand_dims(np.array(seq_data), axis=0)\n",
    "    if total_frame > FIXED_FRAMES:\n",
    "        selected_idx = np.linspace(0, total_frame-1, FIXED_FRAMES, dtype=int)\n",
    "        sequence_landmark = sequence_landmark[:,selected_idx,:]\n",
    "    elif total_frame < FIXED_FRAMES:\n",
    "        sequence_landmark = torch.from_numpy(np.array(sequence_landmark))\n",
    "        sequence_landmark = F.interpolate(sequence_landmark.permute(0,2,1), size=(FIXED_FRAMES), mode= 'nearest-exact').permute(0,2,1).numpy()\n",
    "\n",
    "    return np.squeeze(sequence_landmark, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded = pd.read_csv('balanced_expanded_data_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_IDX, Xval_IDX, Ytrain, Yval = train_test_split(np.arange(len(df_expanded)), df_expanded.sign, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Base Component Dataset\n",
    "(References from Previous Research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee46e2671a84c0781cb096f7b0548f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Data:   0%|          | 0/1021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"x_train\": shape (816, 34, 1662), type \"<f8\">\n",
      "<HDF5 dataset \"y_train\": shape (816,), type \"|O\">\n",
      "<HDF5 dataset \"x_val\": shape (205, 34, 1662), type \"<f8\">\n",
      "<HDF5 dataset \"y_val\": shape (205,), type \"|O\">\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "\n",
    "for i in tqdm.notebook.tnrange(len(df_expanded.path), desc=f\"Landmark Reduce and Frame Equalization\"):\n",
    "    prep_data = landmark_reduction(df_expanded.path[i],\n",
    "                                   faceIDX = np.arange(0,468), \n",
    "                                   poseIDX = np.arange(468,501),\n",
    "                                   rhIDX = np.arange(501,522), \n",
    "                                   lhIDX = np.arange(522,543))\n",
    "    prep_data = frame_equalization(prep_data, FIXED_FRAMES)\n",
    "    x_data.append(prep_data)\n",
    "\n",
    "with h5py.File('data/datasets1.h5','w') as hf:\n",
    "\n",
    "    hf.create_dataset('x_train',\n",
    "                      data=np.take(x_data,Xtrain_IDX,axis=0))\n",
    "    hf.create_dataset('y_train',data=np.array(Ytrain))\n",
    "    hf.create_dataset('x_val',\n",
    "                      data=np.take(x_data,Xval_IDX,axis=0))\n",
    "    hf.create_dataset('y_val',data=np.array(Yval))\n",
    "    print(hf.get('x_train'))\n",
    "    print(hf.get('y_train'))\n",
    "    print(hf.get('x_val'))\n",
    "    print(hf.get('y_val'))\n",
    "\n",
    "del x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Hand, Body, Mouth Dataset Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067368e65a7c431588ebbbfef57adfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Data:   0%|          | 0/1021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"x_train\": shape (816, 34, 378), type \"<f8\">\n",
      "<HDF5 dataset \"y_train\": shape (816,), type \"|O\">\n",
      "<HDF5 dataset \"x_val\": shape (205, 34, 378), type \"<f8\">\n",
      "<HDF5 dataset \"y_val\": shape (205,), type \"|O\">\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "\n",
    "for i in tqdm.notebook.tnrange(len(df_expanded.path), desc=f\"Landmark Reduce and Frame Equalization\"):\n",
    "    prep_data = landmark_reduction(df_expanded.path[i],\n",
    "                                   faceIDX = LIPS_IDX, \n",
    "                                   poseIDX = np.arange(468,501),\n",
    "                                   rhIDX = np.arange(501,522), \n",
    "                                   lhIDX = np.arange(522,543))\n",
    "    prep_data = frame_equalization(prep_data, FIXED_FRAMES)\n",
    "    x_data.append(prep_data)\n",
    "\n",
    "with h5py.File('data/datasets2.h5','w') as hf:\n",
    "\n",
    "    hf.create_dataset('x_train',data=np.take(x_data,Xtrain_IDX,axis=0))\n",
    "    hf.create_dataset('y_train',data=np.array(Ytrain))\n",
    "    hf.create_dataset('x_val',data=np.take(x_data,Xval_IDX,axis=0))\n",
    "    hf.create_dataset('y_val',data=np.array(Yval))\n",
    "    print(hf.get('x_train'))\n",
    "    print(hf.get('y_train'))\n",
    "    print(hf.get('x_val'))\n",
    "    print(hf.get('y_val'))\n",
    "\n",
    "del x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Hand, Upper Body, Face Component Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639ff5f76ece4f31bb9640e01b22809f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Data:   0%|          | 0/1021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"x_train\": shape (816, 34, 1630), type \"<f8\">\n",
      "<HDF5 dataset \"y_train\": shape (816,), type \"|O\">\n",
      "<HDF5 dataset \"x_val\": shape (205, 34, 1630), type \"<f8\">\n",
      "<HDF5 dataset \"y_val\": shape (205,), type \"|O\">\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "\n",
    "for i in tqdm.notebook.tnrange(len(df_expanded.path), desc=f\"Landmark Reduce and Frame Equalization\"):\n",
    "    prep_data = landmark_reduction(df_expanded.path[i],\n",
    "                                   faceIDX = np.arange(0,468), \n",
    "                                   poseIDX = UPPER_BODY_IDX,\n",
    "                                   rhIDX = np.arange(501,522), \n",
    "                                   lhIDX = np.arange(522,543))\n",
    "    prep_data = frame_equalization(prep_data, FIXED_FRAMES)\n",
    "    x_data.append(prep_data)\n",
    "\n",
    "with h5py.File('data/datasets3.h5','w') as hf:\n",
    "\n",
    "    hf.create_dataset('x_train',data=np.take(x_data,Xtrain_IDX,axis=0))\n",
    "    hf.create_dataset('y_train',data=np.array(Ytrain))\n",
    "    hf.create_dataset('x_val',data=np.take(x_data,Xval_IDX,axis=0))\n",
    "    hf.create_dataset('y_val',data=np.array(Yval))\n",
    "    print(hf.get('x_train'))\n",
    "    print(hf.get('y_train'))\n",
    "    print(hf.get('x_val'))\n",
    "    print(hf.get('y_val'))\n",
    "\n",
    "del x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Hand, Upper Body, Mouth Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65a30b2ff664db89e08258263ce40db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Data:   0%|          | 0/1021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"x_train\": shape (816, 34, 346), type \"<f8\">\n",
      "<HDF5 dataset \"y_train\": shape (816,), type \"|O\">\n",
      "<HDF5 dataset \"x_val\": shape (205, 34, 346), type \"<f8\">\n",
      "<HDF5 dataset \"y_val\": shape (205,), type \"|O\">\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "\n",
    "for i in tqdm.notebook.tnrange(len(df_expanded.path), desc=f\"Landmark Reduce and Frame Equalization\"):\n",
    "    prep_data = landmark_reduction(df_expanded.path[i],\n",
    "                                   faceIDX = LIPS_IDX,\n",
    "                                   poseIDX = UPPER_BODY_IDX,\n",
    "                                   rhIDX = np.arange(501,522), \n",
    "                                   lhIDX = np.arange(522,543))\n",
    "    prep_data = frame_equalization(prep_data, FIXED_FRAMES)\n",
    "    x_data.append(prep_data)\n",
    "\n",
    "with h5py.File('data/datasets4.h5','w') as hf:\n",
    "\n",
    "    hf.create_dataset('x_train',data=np.take(x_data,Xtrain_IDX,axis=0))\n",
    "    hf.create_dataset('y_train',data=np.array(Ytrain))\n",
    "    hf.create_dataset('x_val',data=np.take(x_data,Xval_IDX,axis=0))\n",
    "    hf.create_dataset('y_val',data=np.array(Yval))\n",
    "    print(hf.get('x_train'))\n",
    "    print(hf.get('y_train'))\n",
    "    print(hf.get('x_val'))\n",
    "    print(hf.get('y_val'))\n",
    "\n",
    "del x_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Hand Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f407455f2fa647b8be613d88037c5eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing Data:   0%|          | 0/1021 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"x_train\": shape (816, 34, 126), type \"<f8\">\n",
      "<HDF5 dataset \"y_train\": shape (816,), type \"|O\">\n",
      "<HDF5 dataset \"x_val\": shape (205, 34, 126), type \"<f8\">\n",
      "<HDF5 dataset \"y_val\": shape (205,), type \"|O\">\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "\n",
    "for i in tqdm.notebook.tnrange(len(df_expanded.path), desc=f\"Landmark Reduce and Frame Equalization\"):\n",
    "    prep_data = landmark_reduction(df_expanded.path[i],\n",
    "                                   rhIDX = np.arange(501,522), \n",
    "                                   lhIDX = np.arange(522,543))\n",
    "    prep_data = frame_equalization(prep_data, FIXED_FRAMES)\n",
    "    x_data.append(prep_data)\n",
    "\n",
    "\n",
    "with h5py.File('data/datasets5.h5','w') as hf:\n",
    "\n",
    "    hf.create_dataset('x_train',data=np.take(x_data,Xtrain_IDX,axis=0))\n",
    "    hf.create_dataset('y_train',data=np.array(Ytrain))\n",
    "    hf.create_dataset('x_val',data=np.take(x_data,Xval_IDX,axis=0))\n",
    "    hf.create_dataset('y_val',data=np.array(Yval))\n",
    "    print(hf.get('x_train'))\n",
    "    print(hf.get('y_train'))\n",
    "    print(hf.get('x_val'))\n",
    "    print(hf.get('y_val'))\n",
    "\n",
    "del x_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
